{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 11:28:30,960 - INFO - 작업 시작\n",
      "2025-04-18 11:28:30,963 - INFO - 데이터 로드 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 11:28:31,182 - INFO - 학습 데이터 크기: (26099, 20), 테스트 데이터 크기: (6525, 19)\n",
      "2025-04-18 11:28:31,182 - INFO - 데이터 전처리 시작...\n",
      "2025-04-18 11:28:31,192 - INFO - 시간 관련 특성 생성 시작...\n",
      "2025-04-18 11:28:31,251 - INFO - 기준 시작일: 2022-07-01 00:00:00\n",
      "2025-04-18 11:28:31,307 - INFO - 시간 관련 특성 생성 완료\n",
      "2025-04-18 11:28:31,339 - INFO - 역 관련 지역 수: 9202\n",
      "2025-04-18 11:28:31,355 - INFO - 역 관련 지역 수: 2288\n",
      "2025-04-18 11:28:31,386 - INFO - 읍 관련 지역 수: 1259\n",
      "2025-04-18 11:28:31,400 - INFO - 읍 관련 지역 수: 309\n",
      "2025-04-18 11:28:31,508 - INFO - TOTAL_BIDG 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,513 - INFO - FAC_NEIGH_1 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,517 - INFO - FAC_NEIGH_2 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,521 - INFO - FAC_RETAIL 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,525 - INFO - FAC_STAY 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,529 - INFO - FAC_LEISURE 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,534 - INFO - TOTAL_GAS 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,537 - INFO - CMRC_GAS 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,543 - INFO - TOTAL_BIDG 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,547 - INFO - FAC_NEIGH_1 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,549 - INFO - FAC_NEIGH_2 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,565 - INFO - FAC_RETAIL 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,569 - INFO - FAC_STAY 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,571 - INFO - FAC_LEISURE 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,575 - INFO - TOTAL_GAS 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,579 - INFO - CMRC_GAS 변수에 로그 변환 적용\n",
      "2025-04-18 11:28:31,596 - INFO - 범주형 변수 수: 4\n",
      "2025-04-18 11:28:31,598 - INFO - 수치형 변수 수: 28\n",
      "2025-04-18 11:28:33,430 - INFO - 데이터 전처리 완료\n",
      "2025-04-18 11:28:33,432 - INFO - 최종 학습 데이터 특성 수: 1401\n",
      "2025-04-18 11:28:33,434 - INFO - 최종 테스트 데이터 특성 수: 1401\n",
      "2025-04-18 11:28:33,466 - INFO - XGBoost 모델 학습 시작\n",
      "2025-04-18 11:28:33,469 - INFO - 사용할 파라미터: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'gamma': 0, 'random_state': 42}\n",
      "2025-04-18 11:31:23,842 - INFO - 학습 데이터 RMSE (로그 스케일): 0.3202\n",
      "2025-04-18 11:31:23,845 - INFO - 학습 데이터 RMSE (원래 스케일): 544.2676\n",
      "2025-04-18 11:31:23,848 - INFO - 학습 데이터 R² 점수: 0.9597\n",
      "2025-04-18 11:31:23,957 - INFO - 상위 10개 중요 특성:\n",
      "           feature  importance\n",
      "916       AREA_장지역    0.051215\n",
      "627        AREA_신동    0.024163\n",
      "11       TOTAL_GAS    0.021307\n",
      "929       AREA_전주역    0.017672\n",
      "2      FAC_NEIGH_2    0.013318\n",
      "1337      DIST_익산시    0.011540\n",
      "585     AREA_수성동_1    0.010902\n",
      "925     AREA_전북대학교    0.010570\n",
      "1342  DIST_전주시 완산구    0.010103\n",
      "1209      DIST_군산시    0.009466\n",
      "2025-04-18 11:31:26,510 - INFO - 모델이 저장되었습니다: c:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\results/models/XGBoost_optimized_20250418_113126.pkl\n",
      "2025-04-18 11:31:27,316 - INFO - 예측 결과가 저장되었습니다: c:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\results/predictions/XGBoost_optimized_predictions_20250418_113126.csv\n",
      "2025-04-18 11:31:27,319 - INFO - 모든 작업이 성공적으로 완료되었습니다.\n",
      "2025-04-18 11:31:27,322 - INFO - 모델 저장 경로: c:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\results/models/XGBoost_optimized_20250418_113126.pkl\n",
      "2025-04-18 11:31:27,323 - INFO - 예측 결과 저장 경로: c:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\results/predictions/XGBoost_optimized_predictions_20250418_113126.csv\n"
     ]
    }
   ],
   "source": [
    "# 방법 1: 전체 스크립트 실행 (간단하고 모든 함수와 변수가 현재 네임스페이스에 로드됨)\n",
    "%run xgboost_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 11:26:47,892 - INFO - 데이터 로드 시작\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc6 in position 210: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     load_data, \n\u001b[32m      6\u001b[39m     preprocess_data, \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     save_predictions_data\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_df, test_df = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../../data/TRAIN_DATA.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../..data/TEST_DATA.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 데이터 전처리\u001b[39;00m\n\u001b[32m     19\u001b[39m X_train, y_train, X_test, test_df_orig, preprocess_info = preprocess_data(train_df, test_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\notebooks\\LHP\\xgboost_model.py:33\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(train_path, test_path)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     31\u001b[39m     test_path = os.path.join(PROJECT_ROOT, \u001b[33m'\u001b[39m\u001b[33mdata/TEST_DATA.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m test_df = pd.read_csv(test_path)\n\u001b[32m     35\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m학습 데이터 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 테스트 데이터 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_project\\SAS-Korea_Data-Analysis\\SAS-Korea_Data-Analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc6 in position 210: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "\n",
    "# 또는\n",
    "\n",
    "# 방법 2: 필요한 함수만 임포트하여 사용 (모듈화된 방식)\n",
    "from xgboost_model import (\n",
    "    load_data, \n",
    "    preprocess_data, \n",
    "    train_model, \n",
    "    save_model_data, \n",
    "    save_predictions_data\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "train_df, test_df = load_data(\n",
    "    train_path='../../data/TRAIN_DATA.csv',\n",
    "    test_path='../..data/TEST_DATA.csv'\n",
    ")\n",
    "\n",
    "# 데이터 전처리\n",
    "X_train, y_train, X_test, test_df_orig, preprocess_info = preprocess_data(train_df, test_df)\n",
    "\n",
    "# 모델 학습 (지정된 파라미터 사용)\n",
    "model, feature_importance = train_model(X_train, y_train)\n",
    "\n",
    "# 학습 데이터의 성능 확인 (상위 10개 중요 특성)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "predictions_log = model.predict(X_test)\n",
    "\n",
    "# 로그 스케일에서 원래 스케일로 변환\n",
    "import numpy as np\n",
    "predictions = np.expm1(predictions_log)\n",
    "\n",
    "# 모델 저장\n",
    "model_path = save_model_data(model, preprocess_info)\n",
    "\n",
    "# 예측 결과 저장\n",
    "predictions_path = save_predictions_data(test_df_orig, predictions)\n",
    "\n",
    "print(f\"모델 저장 경로: {model_path}\")\n",
    "print(f\"예측 결과 저장 경로: {predictions_path}\")\n",
    "\n",
    "# 결과 확인 (예측값 샘플)\n",
    "import pandas as pd\n",
    "result_df = pd.DataFrame({\n",
    "    'actual': np.expm1(y_train[:5]) if hasattr(y_train, 'iloc') else np.expm1(y_train[:5]),\n",
    "    'predicted': np.expm1(model.predict(X_train[:5]))\n",
    "})\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
