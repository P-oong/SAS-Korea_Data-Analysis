{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72b8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "#설치 확인\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (2.7.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2024.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sally\\anaconda3\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97394c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a41221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DATA_YM  AREA_ID AREA_NM  DIST_CD DIST_NM  TOTAL_BIDG  FAC_NEIGH_1  \\\n",
      "0   202303     9626  중앙로역_4    27110      중구       538.0        266.0   \n",
      "1   202303    10350    황리단길    47130     경주시       765.0         72.0   \n",
      "2   202212     9547    농소1동    31200      북구       343.0         74.0   \n",
      "3   202312     9995  심석고등학교    41360    남양주시       129.0          4.0   \n",
      "4   202301     9248     고덕역    11740     강동구        34.0          2.0   \n",
      "\n",
      "   FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  FAC_RETAIL  FAC_MEDI  FAC_YOSE  \\\n",
      "0         88.0           6.0       3.0         2.0       3.0       0.0   \n",
      "1        174.0           1.0       0.0         2.0       0.0       1.0   \n",
      "2        161.0           1.0       3.0         0.0       2.0       6.0   \n",
      "3         14.0           0.0       1.0         0.0       0.0       0.0   \n",
      "4         14.0           0.0       0.0         1.0       0.0       1.0   \n",
      "\n",
      "   FAC_TRAIN  FAC_SPORT  FAC_STAY  FAC_LEISURE  TOTAL_GAS  CMRC_GAS  \\\n",
      "0        0.0        0.0      16.0          3.0     517520    466070   \n",
      "1        0.0        0.0       2.0          0.0     322681    318143   \n",
      "2        0.0        0.0       0.0          0.0     921843    726861   \n",
      "3        0.0        0.0       0.0          0.0     226095    223952   \n",
      "4        0.0        0.0       0.0          0.0    1884854    886279   \n",
      "\n",
      "   TOTAL_ELEC  \n",
      "0    1174.570  \n",
      "1     571.137  \n",
      "2    1094.982  \n",
      "3     594.064  \n",
      "4    2708.273  \n",
      "   DATA_YM  AREA_ID   AREA_NM  DIST_CD  DIST_NM  TOTAL_BIDG  FAC_NEIGH_1  \\\n",
      "0   202303     9180   종로5가역_1    11110      종로구       196.0         81.0   \n",
      "1   202303    10292       유촌동    29140       서구        17.0          1.0   \n",
      "2   202303     9323  포스코사거리_2    11680      강남구        93.0          9.0   \n",
      "3   202307     9884  장안문로터리_2    41111  수원시 장안구        96.0          9.0   \n",
      "4   202211     9922  포승도곡근린공원    41220      평택시       381.0          6.0   \n",
      "\n",
      "   FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  FAC_RETAIL  FAC_MEDI  FAC_YOSE  \\\n",
      "0        108.0           0.0       0.0         0.0       0.0       0.0   \n",
      "1          1.0           0.0       0.0         0.0       0.0       0.0   \n",
      "2         22.0           0.0       0.0         0.0       0.0       0.0   \n",
      "3         28.0           0.0       0.0         0.0       0.0       0.0   \n",
      "4         20.0           0.0       1.0         1.0       0.0       0.0   \n",
      "\n",
      "   FAC_TRAIN  FAC_SPORT  FAC_STAY  FAC_LEISURE  TOTAL_GAS  CMRC_GAS  \n",
      "0        0.0        0.0       0.0          0.0     143394    143275  \n",
      "1        0.0        0.0       0.0          0.0       3520      2315  \n",
      "2        0.0        0.0       2.0          0.0    1548330    515034  \n",
      "3        0.0        0.0       5.0          0.0      90053     89481  \n",
      "4        0.0        0.0       0.0          2.0     196999     99915  \n"
     ]
    }
   ],
   "source": [
    "#데이터 준비\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('TRAIN_DATA.csv', encoding='euc-kr')\n",
    "test_df = pd.read_csv('TEST_DATA.csv', encoding='euc-kr')\n",
    "\n",
    "#데이터 확인\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df9813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATA_YM', 'AREA_ID', 'AREA_NM', 'DIST_CD', 'DIST_NM', 'TOTAL_BIDG',\n",
      "       'FAC_NEIGH_1', 'FAC_NEIGH_2', 'FAC_CULT_MTG', 'FAC_RELG', 'FAC_RETAIL',\n",
      "       'FAC_MEDI', 'FAC_YOSE', 'FAC_TRAIN', 'FAC_SPORT', 'FAC_STAY',\n",
      "       'FAC_LEISURE', 'TOTAL_GAS', 'CMRC_GAS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d0503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터에만 있는 AREA_ID와 AREA_NM:\n",
      "(9888, '수원시청_1')\n",
      "(9946, '유천아파트앞')\n",
      "\n",
      "Test 데이터에만 있는 AREA_ID와 AREA_NM:\n",
      "\n",
      "Train 데이터에만 있는 DIST_CD와 DIST_NM:\n",
      "(47720, '군위군')\n",
      "(42720, '홍천군')\n",
      "(42730, '횡성군')\n",
      "\n",
      "Test 데이터에만 있는 DIST_CD와 DIST_NM:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AREA_ID와 AREA_NM 컬럼 리스트로 추출\n",
    "train_areas = train_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "test_areas = test_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "\n",
    "# AREA_ID와 AREA_NM을 기준으로 겹치지 않는 값 찾기\n",
    "train_ids = set(zip(train_areas['AREA_ID'], train_areas['AREA_NM']))\n",
    "test_ids = set(zip(test_areas['AREA_ID'], test_areas['AREA_NM']))\n",
    "\n",
    "# 겹치지 않는 값 찾기\n",
    "train_unique = train_ids - test_ids\n",
    "test_unique = test_ids - train_ids\n",
    "\n",
    "# DIST_CD와 DIST_NM 컬럼 리스트로 추출\n",
    "train_dist = train_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "test_dist = test_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "\n",
    "# DIST_CD와 DIST_NM 기준으로 겹치지 않는 값 찾기\n",
    "train_cds = set(zip(train_dist['DIST_CD'], train_dist['DIST_NM']))\n",
    "test_cds = set(zip(test_dist['DIST_CD'], test_dist['DIST_NM']))\n",
    "\n",
    "# 겹치지 않는 값 찾기\n",
    "train_unique2 = train_cds - test_cds\n",
    "test_unique2 = test_cds - train_cds\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train 데이터에만 있는 AREA_ID와 AREA_NM:\")\n",
    "for area in train_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest 데이터에만 있는 AREA_ID와 AREA_NM:\")\n",
    "for area in test_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTrain 데이터에만 있는 DIST_CD와 DIST_NM:\")\n",
    "for area in train_unique2:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest 데이터에만 있는 DIST_CD와 DIST_NM:\")\n",
    "for area in test_unique2:\n",
    "    print(area)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6a1782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 출력되는게 없으면 성공 =====\n",
      "\n",
      "Train 데이터에만 있는 AREA_ID와 AREA_NM:\n",
      "\n",
      "Test 데이터에만 있는 AREA_ID와 AREA_NM:\n",
      "\n",
      "Train 데이터에만 있는 DIST_CD와 DIST_NM:\n",
      "\n",
      "Test 데이터에만 있는 DIST_CD와 DIST_NM:\n"
     ]
    }
   ],
   "source": [
    "# Train 데이터에만 있는 값들\n",
    "train_remove_ids = [(9888, '수원시청_1'), (9946, '유천아파트앞')]\n",
    "train_remove_dist = [(42720, '홍천군'), (42730, '횡성군'), (47720, '군위군')]\n",
    "\n",
    "# Train 데이터에서 삭제\n",
    "for area_id, area_name in train_remove_ids:\n",
    "    train_df = train_df[~((train_df['AREA_ID'] == area_id) & (train_df['AREA_NM'] == area_name))]\n",
    "\n",
    "for dist_cd, dist_name in train_remove_dist:\n",
    "    train_df = train_df[~((train_df['DIST_CD'] == dist_cd) & (train_df['DIST_NM'] == dist_name))]\n",
    "\n",
    "# 확인하기!!\n",
    "print(\"===== 출력되는게 없으면 성공 =====\")\n",
    "# AREA_ID와 AREA_NM 컬럼 리스트로 추출\n",
    "train_areas = train_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "test_areas = test_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "\n",
    "# AREA_ID와 AREA_NM을 기준으로 겹치지 않는 값 찾기\n",
    "train_ids = set(zip(train_areas['AREA_ID'], train_areas['AREA_NM']))\n",
    "test_ids = set(zip(test_areas['AREA_ID'], test_areas['AREA_NM']))\n",
    "\n",
    "# 겹치지 않는 값 찾기\n",
    "train_unique = train_ids - test_ids\n",
    "test_unique = test_ids - train_ids\n",
    "\n",
    "# DIST_CD와 DIST_NM 컬럼 리스트로 추출\n",
    "train_dist = train_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "test_dist = test_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "\n",
    "# DIST_CD와 DIST_NM 기준으로 겹치지 않는 값 찾기\n",
    "train_cds = set(zip(train_dist['DIST_CD'], train_dist['DIST_NM']))\n",
    "test_cds = set(zip(test_dist['DIST_CD'], test_dist['DIST_NM']))\n",
    "\n",
    "# 겹치지 않는 값 찾기\n",
    "train_unique2 = train_cds - test_cds\n",
    "test_unique2 = test_cds - train_cds\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nTrain 데이터에만 있는 AREA_ID와 AREA_NM:\")\n",
    "for area in train_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest 데이터에만 있는 AREA_ID와 AREA_NM:\")\n",
    "for area in test_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTrain 데이터에만 있는 DIST_CD와 DIST_NM:\")\n",
    "for area in train_unique2:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest 데이터에만 있는 DIST_CD와 DIST_NM:\")\n",
    "for area in test_unique2:\n",
    "    print(area)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf512de0",
   "metadata": {},
   "source": [
    "## 범주값 처리\n",
    "1. 일단 원-핫 인코딩 하자.\n",
    "2. 원-핫 인코딩 후 입벧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2091da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DATA_YM  TOTAL_BIDG  FAC_NEIGH_1  FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  \\\n",
      "0   202303       538.0        266.0         88.0           6.0       3.0   \n",
      "1   202303       765.0         72.0        174.0           1.0       0.0   \n",
      "2   202212       343.0         74.0        161.0           1.0       3.0   \n",
      "3   202312       129.0          4.0         14.0           0.0       1.0   \n",
      "4   202301        34.0          2.0         14.0           0.0       0.0   \n",
      "\n",
      "   FAC_RETAIL  FAC_MEDI  FAC_YOSE  FAC_TRAIN  ...  DIST_NM_중구  \\\n",
      "0         2.0       3.0       0.0        0.0  ...        True   \n",
      "1         2.0       0.0       1.0        0.0  ...       False   \n",
      "2         0.0       2.0       6.0        0.0  ...       False   \n",
      "3         0.0       0.0       0.0        0.0  ...       False   \n",
      "4         1.0       0.0       1.0        0.0  ...       False   \n",
      "\n",
      "   DIST_NM_창원시 성산구  DIST_NM_철원군  DIST_NM_청주시 서원구  DIST_NM_청주시 흥덕구  \\\n",
      "0            False        False            False            False   \n",
      "1            False        False            False            False   \n",
      "2            False        False            False            False   \n",
      "3            False        False            False            False   \n",
      "4            False        False            False            False   \n",
      "\n",
      "   DIST_NM_춘천시  DIST_NM_태백시  DIST_NM_포항시 남구  DIST_NM_해운대구  DIST_NM_화천군  \n",
      "0        False        False           False         False        False  \n",
      "1        False        False           False         False        False  \n",
      "2        False        False           False         False        False  \n",
      "3        False        False           False         False        False  \n",
      "4        False        False           False         False        False  \n",
      "\n",
      "[5 rows x 158 columns]\n"
     ]
    }
   ],
   "source": [
    "# AREA_ID와 DIST_CD 제거\n",
    "train_df = train_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "# AREA_NM에 대한 처리\n",
    "top_100_area_nm = train_df.nlargest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "bottom_100_area_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "\n",
    "# DIST_NM에 대한 처리\n",
    "top_100_dist_nm = train_df.nlargest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "bottom_100_dist_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "\n",
    "# 중요 범주를 표시하고 나머지는 'Other'로 묶기\n",
    "train_df['AREA_NM'] = train_df['AREA_NM'].where(train_df['AREA_NM'].isin(top_100_area_nm) | train_df['AREA_NM'].isin(bottom_100_area_nm), 'Other')\n",
    "\n",
    "train_df['DIST_NM'] = train_df['DIST_NM'].where(train_df['DIST_NM'].isin(top_100_dist_nm) | train_df['DIST_NM'].isin(bottom_100_dist_nm), 'Other')\n",
    "\n",
    "# 원-핫 인코딩 적용 (drop_first=True는 첫 번째 카테고리를 제외)\n",
    "data = pd.get_dummies(train_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08530434",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e703a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(train_df):\n",
    "    # 제거할 ID 및 DIST 목록\n",
    "    train_remove_ids = [(9888, '수원시청_1'), (9946, '유천아파트앞')]\n",
    "    train_remove_dist = [(42720, '홍천군'), (42730, '횡성군'), (47720, '군위군')]\n",
    "\n",
    "    for area_id, area_name in train_remove_ids:\n",
    "        train_df = train_df[~((train_df['AREA_ID'] == area_id) & (train_df['AREA_NM'] == area_name))]\n",
    "\n",
    "    for dist_cd, dist_name in train_remove_dist:\n",
    "        train_df = train_df[~((train_df['DIST_CD'] == dist_cd) & (train_df['DIST_NM'] == dist_name))]\n",
    "\n",
    "    # 컬럼 제거\n",
    "    train_df = train_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "    # 중요 범주 추출\n",
    "    top_100_area_nm = train_df.nlargest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "    bottom_100_area_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "    top_100_dist_nm = train_df.nlargest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "    bottom_100_dist_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "\n",
    "    # 범주 압축\n",
    "    train_df['AREA_NM'] = train_df['AREA_NM'].where(train_df['AREA_NM'].isin(top_100_area_nm) | train_df['AREA_NM'].isin(bottom_100_area_nm), 'Other')\n",
    "    train_df['DIST_NM'] = train_df['DIST_NM'].where(train_df['DIST_NM'].isin(top_100_dist_nm) | train_df['DIST_NM'].isin(bottom_100_dist_nm), 'Other')\n",
    "\n",
    "    # NaN 처리\n",
    "    num_cols = train_df.select_dtypes(include='number').columns\n",
    "    cat_cols = train_df.select_dtypes(include='object').columns\n",
    "\n",
    "    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].mean())\n",
    "    train_df[cat_cols] = train_df[cat_cols].fillna('Other')\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    train_df_encoded = pd.get_dummies(train_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "    # return: 전처리된 데이터, top/bottom 범주 목록 (테스트에 활용)\n",
    "    return train_df_encoded, top_100_area_nm, bottom_100_area_nm, top_100_dist_nm, bottom_100_dist_nm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_df, top_areas, bottom_areas, top_dists, bottom_dists, reference_columns):\n",
    "    # 동일한 컬럼 제거\n",
    "    test_df = test_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "    # 범주 압축\n",
    "    test_df['AREA_NM'] = test_df['AREA_NM'].where(test_df['AREA_NM'].isin(top_areas) | test_df['AREA_NM'].isin(bottom_areas), 'Other')\n",
    "    test_df['DIST_NM'] = test_df['DIST_NM'].where(test_df['DIST_NM'].isin(top_dists) | test_df['DIST_NM'].isin(bottom_dists), 'Other')\n",
    "\n",
    "    # NaN 처리\n",
    "    num_cols = test_df.select_dtypes(include='number').columns\n",
    "    cat_cols = test_df.select_dtypes(include='object').columns\n",
    "\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].mean())\n",
    "    test_df[cat_cols] = test_df[cat_cols].fillna('Other')\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    test_df_encoded = pd.get_dummies(test_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "    # 누락된 컬럼 채우고 순서 맞춤\n",
    "    missing_cols = set(reference_columns) - set(test_df_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        test_df_encoded[col] = 0\n",
    "\n",
    "    test_df_encoded = test_df_encoded[reference_columns]\n",
    "\n",
    "    return test_df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef51336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/42\n",
      " ➤ Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 39.48096| val_rmse: 5.34528 |  0:00:03s\n",
      "epoch 1  | loss: 38.83648| val_rmse: 8.72288 |  0:00:07s\n",
      "epoch 2  | loss: 38.07483| val_rmse: 6.67571 |  0:00:11s\n",
      "epoch 3  | loss: 37.39881| val_rmse: 6.92304 |  0:00:15s\n",
      "epoch 4  | loss: 36.60314| val_rmse: 6.22639 |  0:00:19s\n",
      "epoch 5  | loss: 35.97552| val_rmse: 6.03657 |  0:00:23s\n",
      "epoch 6  | loss: 35.27765| val_rmse: 5.89713 |  0:00:27s\n",
      "epoch 7  | loss: 34.47963| val_rmse: 6.13468 |  0:00:31s\n",
      "epoch 8  | loss: 33.69906| val_rmse: 6.22045 |  0:00:36s\n",
      "epoch 9  | loss: 32.98782| val_rmse: 6.19184 |  0:00:40s\n",
      "epoch 10 | loss: 32.11109| val_rmse: 6.11273 |  0:00:43s\n",
      "epoch 11 | loss: 31.3971 | val_rmse: 6.06413 |  0:00:47s\n",
      "epoch 12 | loss: 30.56873| val_rmse: 6.03026 |  0:00:51s\n",
      "epoch 13 | loss: 29.69859| val_rmse: 5.95094 |  0:00:55s\n",
      "epoch 14 | loss: 28.72013| val_rmse: 5.8878  |  0:01:00s\n",
      "epoch 15 | loss: 27.91503| val_rmse: 5.78697 |  0:01:05s\n",
      "epoch 16 | loss: 26.88939| val_rmse: 5.77746 |  0:01:09s\n",
      "epoch 17 | loss: 25.85254| val_rmse: 5.649   |  0:01:13s\n",
      "epoch 18 | loss: 25.08571| val_rmse: 5.57322 |  0:01:18s\n",
      "epoch 19 | loss: 24.27272| val_rmse: 5.53329 |  0:01:23s\n",
      "epoch 20 | loss: 23.22005| val_rmse: 5.39113 |  0:01:27s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_rmse = 5.34528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39miloc[val_idx]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtabnet_params)\n\u001b[1;32m---> 63\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     64\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train, y_train\u001b[38;5;241m=\u001b[39my_train,\n\u001b[0;32m     65\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m     66\u001b[0m     eval_name\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     67\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     68\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     69\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     70\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, virtual_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     71\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     72\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     76\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, preds, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:278\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_importance:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# compute feature importance once the best model is defined\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_importances_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_feature_importances(X_train)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:759\u001b[0m, in \u001b[0;36mTabModel._compute_feature_importances\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_feature_importances\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute global feature importance.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     M_explain, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(X, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    760\u001b[0m     sum_explain \u001b[38;5;241m=\u001b[39m M_explain\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    761\u001b[0m     feature_importances_ \u001b[38;5;241m=\u001b[39m sum_explain \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sum_explain)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:353\u001b[0m, in \u001b[0;36mTabModel.explain\u001b[1;34m(self, X, normalize)\u001b[0m\n\u001b[0;32m    345\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    346\u001b[0m         PredictDataset(X),\n\u001b[0;32m    347\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    348\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m     )\n\u001b[0;32m    351\u001b[0m res_explain \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    354\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    356\u001b[0m     M_explain, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mforward_masks(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:283\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "# 시드 설정\n",
    "np.random.seed(0)\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv('TRAIN_DATA.csv', encoding='euc-kr')\n",
    "test_df = pd.read_csv('TEST_DATA.csv', encoding='euc-kr')\n",
    "\n",
    "# 전처리 (함수 이용)\n",
    "train_data_processed, top_areas, bottom_areas, top_dists, bottom_dists = preprocess_train_data(train_df)\n",
    "\n",
    "# 타겟 변수 로그 변환\n",
    "train_data_processed['TOTAL_ELEC'] = np.log1p(train_data_processed['TOTAL_ELEC'])\n",
    "\n",
    "# 예측용 X, y\n",
    "X = train_data_processed.drop(columns='TOTAL_ELEC')\n",
    "y = train_data_processed['TOTAL_ELEC']\n",
    "\n",
    "# 테스트 데이터 전처리\n",
    "test_data_processed = preprocess_test_data(test_df, top_areas, bottom_areas, top_dists, bottom_dists, X.columns)\n",
    "\n",
    "# 원-핫 인코딩을 했기 때문에 범주형 인덱스는 비워둠\n",
    "cat_idxs = []\n",
    "cat_dims = []\n",
    "\n",
    "# 모델 파라미터\n",
    "tabnet_params = {\n",
    "    \"cat_idxs\": cat_idxs,\n",
    "    \"cat_dims\": cat_dims,\n",
    "    \"cat_emb_dim\": 1,\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": dict(lr=1e-4),\n",
    "    \"scheduler_params\": {\"step_size\": 50, \"gamma\": 0.9},\n",
    "    \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n",
    "    \"mask_type\": \"entmax\",\n",
    "    \"gamma\": 1.3,\n",
    "    #\"loss_fn\": \"logcosh\"\n",
    "}\n",
    "\n",
    "# 반복 및 K-Fold 설정\n",
    "num_repeats = 42\n",
    "num_folds = 5\n",
    "rmse_scores = []\n",
    "y_pred_final = []  # 최종 예측값을 저장할 리스트\n",
    "\n",
    "for repeat in range(num_repeats):\n",
    "    print(f\"\\n Repeat {repeat + 1}/{num_repeats}\")\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=repeat)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\" ➤ Fold {fold + 1}/{num_folds}\")\n",
    "        X_train, X_val = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "        y_train, y_val = y.iloc[train_idx].values.reshape(-1, 1), y.iloc[val_idx].values.reshape(-1, 1)\n",
    "\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_name=['val'],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=100,\n",
    "            patience=20,\n",
    "            batch_size=256, virtual_batch_size=64,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val).flatten()\n",
    "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "# 전체 반복이 끝난 후, y_pred_final에 모든 예측값이 저장됨\n",
    "y_pred_final = np.concatenate(y_pred_final, axis=0)  # 모든 fold와 repeat에서 나온 예측값을 결합\n",
    "\n",
    "print(f\"\\n 평균 RMSE (Across {num_repeats} repeats x {num_folds} folds): {np.mean(rmse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab9eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 평균 RMSE (Across 42 repeats x 5 folds): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ 평균 RMSE (Across {num_repeats} repeats x {num_folds} folds): {np.mean(rmse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf7ccf",
   "metadata": {},
   "source": [
    "#### ✅ 상권별 예측 오차 분석 (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2b8ec92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 예측값 계산\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_pred_final \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# 예측 결과를 y_pred_final에 저장\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 예측 오차 계산\u001b[39;00m\n\u001b[0;32m      8\u001b[0m error_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:310\u001b[0m, in \u001b[0;36mTabModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    304\u001b[0m         PredictDataset(X),\n\u001b[0;32m    305\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    306\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    309\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    311\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    312\u001b[0m     output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\utils.py:78\u001b[0m, in \u001b[0;36mPredictDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[index]\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 예측값 계산\n",
    "y_pred_final = model.predict(X).flatten()  # 예측 결과를 y_pred_final에 저장\n",
    "\n",
    "# 예측 오차 계산\n",
    "error_df = train_df.copy()\n",
    "error_df['predicted'] = y_pred_final\n",
    "error_df['abs_error'] = np.abs(error_df['TOTAL_ELEC'] - error_df['predicted'])\n",
    "\n",
    "# 상위 10개 상권 표시 (오차 기준)\n",
    "top_errors = error_df.sort_values('abs_error', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_errors, x='AREA_NM', y='abs_error', palette='Reds_r')\n",
    "plt.title(\"상권별 예측 오차 상위 10\")\n",
    "plt.xlabel(\"상권 명\")\n",
    "plt.ylabel(\"절대 오차 (abs_error)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571824b",
   "metadata": {},
   "source": [
    "#### ✅ 전체 학습 데이터로 최종 모델 재학습\n",
    "- 전체 X, y로 최종 TabNet 모델 재훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04206c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 학습 데이터로 모델 학습\n",
    "final_model = TabNetRegressor(**tabnet_params)\n",
    "final_model.fit(\n",
    "    X_train=X.values, y_train=y.values,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행\n",
    "y_pred = final_model.predict(test_data_processed.values).flatten()\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "submission = pd.DataFrame({'y_pred': y_pred})\n",
    "\n",
    "# 저장\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"예측 결과가 'submission.csv' 로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
