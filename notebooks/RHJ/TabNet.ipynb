{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72b8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "#ì„¤ì¹˜ í™•ì¸\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (2.7.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sally\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2024.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sally\\anaconda3\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sally\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97394c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a41221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DATA_YM  AREA_ID AREA_NM  DIST_CD DIST_NM  TOTAL_BIDG  FAC_NEIGH_1  \\\n",
      "0   202303     9626  ì¤‘ì•™ë¡œì—­_4    27110      ì¤‘êµ¬       538.0        266.0   \n",
      "1   202303    10350    í™©ë¦¬ë‹¨ê¸¸    47130     ê²½ì£¼ì‹œ       765.0         72.0   \n",
      "2   202212     9547    ë†ì†Œ1ë™    31200      ë¶êµ¬       343.0         74.0   \n",
      "3   202312     9995  ì‹¬ì„ê³ ë“±í•™êµ    41360    ë‚¨ì–‘ì£¼ì‹œ       129.0          4.0   \n",
      "4   202301     9248     ê³ ë•ì—­    11740     ê°•ë™êµ¬        34.0          2.0   \n",
      "\n",
      "   FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  FAC_RETAIL  FAC_MEDI  FAC_YOSE  \\\n",
      "0         88.0           6.0       3.0         2.0       3.0       0.0   \n",
      "1        174.0           1.0       0.0         2.0       0.0       1.0   \n",
      "2        161.0           1.0       3.0         0.0       2.0       6.0   \n",
      "3         14.0           0.0       1.0         0.0       0.0       0.0   \n",
      "4         14.0           0.0       0.0         1.0       0.0       1.0   \n",
      "\n",
      "   FAC_TRAIN  FAC_SPORT  FAC_STAY  FAC_LEISURE  TOTAL_GAS  CMRC_GAS  \\\n",
      "0        0.0        0.0      16.0          3.0     517520    466070   \n",
      "1        0.0        0.0       2.0          0.0     322681    318143   \n",
      "2        0.0        0.0       0.0          0.0     921843    726861   \n",
      "3        0.0        0.0       0.0          0.0     226095    223952   \n",
      "4        0.0        0.0       0.0          0.0    1884854    886279   \n",
      "\n",
      "   TOTAL_ELEC  \n",
      "0    1174.570  \n",
      "1     571.137  \n",
      "2    1094.982  \n",
      "3     594.064  \n",
      "4    2708.273  \n",
      "   DATA_YM  AREA_ID   AREA_NM  DIST_CD  DIST_NM  TOTAL_BIDG  FAC_NEIGH_1  \\\n",
      "0   202303     9180   ì¢…ë¡œ5ê°€ì—­_1    11110      ì¢…ë¡œêµ¬       196.0         81.0   \n",
      "1   202303    10292       ìœ ì´Œë™    29140       ì„œêµ¬        17.0          1.0   \n",
      "2   202303     9323  í¬ìŠ¤ì½”ì‚¬ê±°ë¦¬_2    11680      ê°•ë‚¨êµ¬        93.0          9.0   \n",
      "3   202307     9884  ìž¥ì•ˆë¬¸ë¡œí„°ë¦¬_2    41111  ìˆ˜ì›ì‹œ ìž¥ì•ˆêµ¬        96.0          9.0   \n",
      "4   202211     9922  í¬ìŠ¹ë„ê³¡ê·¼ë¦°ê³µì›    41220      í‰íƒì‹œ       381.0          6.0   \n",
      "\n",
      "   FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  FAC_RETAIL  FAC_MEDI  FAC_YOSE  \\\n",
      "0        108.0           0.0       0.0         0.0       0.0       0.0   \n",
      "1          1.0           0.0       0.0         0.0       0.0       0.0   \n",
      "2         22.0           0.0       0.0         0.0       0.0       0.0   \n",
      "3         28.0           0.0       0.0         0.0       0.0       0.0   \n",
      "4         20.0           0.0       1.0         1.0       0.0       0.0   \n",
      "\n",
      "   FAC_TRAIN  FAC_SPORT  FAC_STAY  FAC_LEISURE  TOTAL_GAS  CMRC_GAS  \n",
      "0        0.0        0.0       0.0          0.0     143394    143275  \n",
      "1        0.0        0.0       0.0          0.0       3520      2315  \n",
      "2        0.0        0.0       2.0          0.0    1548330    515034  \n",
      "3        0.0        0.0       5.0          0.0      90053     89481  \n",
      "4        0.0        0.0       0.0          2.0     196999     99915  \n"
     ]
    }
   ],
   "source": [
    "#ë°ì´í„° ì¤€ë¹„\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('TRAIN_DATA.csv', encoding='euc-kr')\n",
    "test_df = pd.read_csv('TEST_DATA.csv', encoding='euc-kr')\n",
    "\n",
    "#ë°ì´í„° í™•ì¸\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df9813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATA_YM', 'AREA_ID', 'AREA_NM', 'DIST_CD', 'DIST_NM', 'TOTAL_BIDG',\n",
      "       'FAC_NEIGH_1', 'FAC_NEIGH_2', 'FAC_CULT_MTG', 'FAC_RELG', 'FAC_RETAIL',\n",
      "       'FAC_MEDI', 'FAC_YOSE', 'FAC_TRAIN', 'FAC_SPORT', 'FAC_STAY',\n",
      "       'FAC_LEISURE', 'TOTAL_GAS', 'CMRC_GAS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d0503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\n",
      "(9888, 'ìˆ˜ì›ì‹œì²­_1')\n",
      "(9946, 'ìœ ì²œì•„íŒŒíŠ¸ì•ž')\n",
      "\n",
      "Test ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\n",
      "\n",
      "Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\n",
      "(47720, 'êµ°ìœ„êµ°')\n",
      "(42720, 'í™ì²œêµ°')\n",
      "(42730, 'íš¡ì„±êµ°')\n",
      "\n",
      "Test ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AREA_IDì™€ AREA_NM ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ\n",
    "train_areas = train_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "test_areas = test_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "\n",
    "# AREA_IDì™€ AREA_NMì„ ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_ids = set(zip(train_areas['AREA_ID'], train_areas['AREA_NM']))\n",
    "test_ids = set(zip(test_areas['AREA_ID'], test_areas['AREA_NM']))\n",
    "\n",
    "# ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_unique = train_ids - test_ids\n",
    "test_unique = test_ids - train_ids\n",
    "\n",
    "# DIST_CDì™€ DIST_NM ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ\n",
    "train_dist = train_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "test_dist = test_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "\n",
    "# DIST_CDì™€ DIST_NM ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_cds = set(zip(train_dist['DIST_CD'], train_dist['DIST_NM']))\n",
    "test_cds = set(zip(test_dist['DIST_CD'], test_dist['DIST_NM']))\n",
    "\n",
    "# ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_unique2 = train_cds - test_cds\n",
    "test_unique2 = test_cds - train_cds\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\")\n",
    "for area in train_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\")\n",
    "for area in test_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTrain ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\")\n",
    "for area in train_unique2:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\")\n",
    "for area in test_unique2:\n",
    "    print(area)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6a1782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ì¶œë ¥ë˜ëŠ”ê²Œ ì—†ìœ¼ë©´ ì„±ê³µ =====\n",
      "\n",
      "Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\n",
      "\n",
      "Test ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\n",
      "\n",
      "Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\n",
      "\n",
      "Test ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\n"
     ]
    }
   ],
   "source": [
    "# Train ë°ì´í„°ì—ë§Œ ìžˆëŠ” ê°’ë“¤\n",
    "train_remove_ids = [(9888, 'ìˆ˜ì›ì‹œì²­_1'), (9946, 'ìœ ì²œì•„íŒŒíŠ¸ì•ž')]\n",
    "train_remove_dist = [(42720, 'í™ì²œêµ°'), (42730, 'íš¡ì„±êµ°'), (47720, 'êµ°ìœ„êµ°')]\n",
    "\n",
    "# Train ë°ì´í„°ì—ì„œ ì‚­ì œ\n",
    "for area_id, area_name in train_remove_ids:\n",
    "    train_df = train_df[~((train_df['AREA_ID'] == area_id) & (train_df['AREA_NM'] == area_name))]\n",
    "\n",
    "for dist_cd, dist_name in train_remove_dist:\n",
    "    train_df = train_df[~((train_df['DIST_CD'] == dist_cd) & (train_df['DIST_NM'] == dist_name))]\n",
    "\n",
    "# í™•ì¸í•˜ê¸°!!\n",
    "print(\"===== ì¶œë ¥ë˜ëŠ”ê²Œ ì—†ìœ¼ë©´ ì„±ê³µ =====\")\n",
    "# AREA_IDì™€ AREA_NM ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ\n",
    "train_areas = train_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "test_areas = test_df[['AREA_ID', 'AREA_NM']].drop_duplicates()\n",
    "\n",
    "# AREA_IDì™€ AREA_NMì„ ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_ids = set(zip(train_areas['AREA_ID'], train_areas['AREA_NM']))\n",
    "test_ids = set(zip(test_areas['AREA_ID'], test_areas['AREA_NM']))\n",
    "\n",
    "# ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_unique = train_ids - test_ids\n",
    "test_unique = test_ids - train_ids\n",
    "\n",
    "# DIST_CDì™€ DIST_NM ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ\n",
    "train_dist = train_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "test_dist = test_df[['DIST_CD', 'DIST_NM']].drop_duplicates()\n",
    "\n",
    "# DIST_CDì™€ DIST_NM ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_cds = set(zip(train_dist['DIST_CD'], train_dist['DIST_NM']))\n",
    "test_cds = set(zip(test_dist['DIST_CD'], test_dist['DIST_NM']))\n",
    "\n",
    "# ê²¹ì¹˜ì§€ ì•ŠëŠ” ê°’ ì°¾ê¸°\n",
    "train_unique2 = train_cds - test_cds\n",
    "test_unique2 = test_cds - train_cds\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nTrain ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\")\n",
    "for area in train_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest ë°ì´í„°ì—ë§Œ ìžˆëŠ” AREA_IDì™€ AREA_NM:\")\n",
    "for area in test_unique:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTrain ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\")\n",
    "for area in train_unique2:\n",
    "    print(area)\n",
    "\n",
    "print(\"\\nTest ë°ì´í„°ì—ë§Œ ìžˆëŠ” DIST_CDì™€ DIST_NM:\")\n",
    "for area in test_unique2:\n",
    "    print(area)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf512de0",
   "metadata": {},
   "source": [
    "## ë²”ì£¼ê°’ ì²˜ë¦¬\n",
    "1. ì¼ë‹¨ ì›-í•« ì¸ì½”ë”© í•˜ìž.\n",
    "2. ì›-í•« ì¸ì½”ë”© í›„ ìž…ë²§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2091da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DATA_YM  TOTAL_BIDG  FAC_NEIGH_1  FAC_NEIGH_2  FAC_CULT_MTG  FAC_RELG  \\\n",
      "0   202303       538.0        266.0         88.0           6.0       3.0   \n",
      "1   202303       765.0         72.0        174.0           1.0       0.0   \n",
      "2   202212       343.0         74.0        161.0           1.0       3.0   \n",
      "3   202312       129.0          4.0         14.0           0.0       1.0   \n",
      "4   202301        34.0          2.0         14.0           0.0       0.0   \n",
      "\n",
      "   FAC_RETAIL  FAC_MEDI  FAC_YOSE  FAC_TRAIN  ...  DIST_NM_ì¤‘êµ¬  \\\n",
      "0         2.0       3.0       0.0        0.0  ...        True   \n",
      "1         2.0       0.0       1.0        0.0  ...       False   \n",
      "2         0.0       2.0       6.0        0.0  ...       False   \n",
      "3         0.0       0.0       0.0        0.0  ...       False   \n",
      "4         1.0       0.0       1.0        0.0  ...       False   \n",
      "\n",
      "   DIST_NM_ì°½ì›ì‹œ ì„±ì‚°êµ¬  DIST_NM_ì² ì›êµ°  DIST_NM_ì²­ì£¼ì‹œ ì„œì›êµ¬  DIST_NM_ì²­ì£¼ì‹œ í¥ë•êµ¬  \\\n",
      "0            False        False            False            False   \n",
      "1            False        False            False            False   \n",
      "2            False        False            False            False   \n",
      "3            False        False            False            False   \n",
      "4            False        False            False            False   \n",
      "\n",
      "   DIST_NM_ì¶˜ì²œì‹œ  DIST_NM_íƒœë°±ì‹œ  DIST_NM_í¬í•­ì‹œ ë‚¨êµ¬  DIST_NM_í•´ìš´ëŒ€êµ¬  DIST_NM_í™”ì²œêµ°  \n",
      "0        False        False           False         False        False  \n",
      "1        False        False           False         False        False  \n",
      "2        False        False           False         False        False  \n",
      "3        False        False           False         False        False  \n",
      "4        False        False           False         False        False  \n",
      "\n",
      "[5 rows x 158 columns]\n"
     ]
    }
   ],
   "source": [
    "# AREA_IDì™€ DIST_CD ì œê±°\n",
    "train_df = train_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "# AREA_NMì— ëŒ€í•œ ì²˜ë¦¬\n",
    "top_100_area_nm = train_df.nlargest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "bottom_100_area_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "\n",
    "# DIST_NMì— ëŒ€í•œ ì²˜ë¦¬\n",
    "top_100_dist_nm = train_df.nlargest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "bottom_100_dist_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "\n",
    "# ì¤‘ìš” ë²”ì£¼ë¥¼ í‘œì‹œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 'Other'ë¡œ ë¬¶ê¸°\n",
    "train_df['AREA_NM'] = train_df['AREA_NM'].where(train_df['AREA_NM'].isin(top_100_area_nm) | train_df['AREA_NM'].isin(bottom_100_area_nm), 'Other')\n",
    "\n",
    "train_df['DIST_NM'] = train_df['DIST_NM'].where(train_df['DIST_NM'].isin(top_100_dist_nm) | train_df['DIST_NM'].isin(bottom_100_dist_nm), 'Other')\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”© ì ìš© (drop_first=TrueëŠ” ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì œì™¸)\n",
    "data = pd.get_dummies(train_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08530434",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e703a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(train_df):\n",
    "    # ì œê±°í•  ID ë° DIST ëª©ë¡\n",
    "    train_remove_ids = [(9888, 'ìˆ˜ì›ì‹œì²­_1'), (9946, 'ìœ ì²œì•„íŒŒíŠ¸ì•ž')]\n",
    "    train_remove_dist = [(42720, 'í™ì²œêµ°'), (42730, 'íš¡ì„±êµ°'), (47720, 'êµ°ìœ„êµ°')]\n",
    "\n",
    "    for area_id, area_name in train_remove_ids:\n",
    "        train_df = train_df[~((train_df['AREA_ID'] == area_id) & (train_df['AREA_NM'] == area_name))]\n",
    "\n",
    "    for dist_cd, dist_name in train_remove_dist:\n",
    "        train_df = train_df[~((train_df['DIST_CD'] == dist_cd) & (train_df['DIST_NM'] == dist_name))]\n",
    "\n",
    "    # ì»¬ëŸ¼ ì œê±°\n",
    "    train_df = train_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "    # ì¤‘ìš” ë²”ì£¼ ì¶”ì¶œ\n",
    "    top_100_area_nm = train_df.nlargest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "    bottom_100_area_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['AREA_NM'].unique()\n",
    "    top_100_dist_nm = train_df.nlargest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "    bottom_100_dist_nm = train_df.nsmallest(100, 'TOTAL_ELEC')['DIST_NM'].unique()\n",
    "\n",
    "    # ë²”ì£¼ ì••ì¶•\n",
    "    train_df['AREA_NM'] = train_df['AREA_NM'].where(train_df['AREA_NM'].isin(top_100_area_nm) | train_df['AREA_NM'].isin(bottom_100_area_nm), 'Other')\n",
    "    train_df['DIST_NM'] = train_df['DIST_NM'].where(train_df['DIST_NM'].isin(top_100_dist_nm) | train_df['DIST_NM'].isin(bottom_100_dist_nm), 'Other')\n",
    "\n",
    "    # NaN ì²˜ë¦¬\n",
    "    num_cols = train_df.select_dtypes(include='number').columns\n",
    "    cat_cols = train_df.select_dtypes(include='object').columns\n",
    "\n",
    "    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].mean())\n",
    "    train_df[cat_cols] = train_df[cat_cols].fillna('Other')\n",
    "\n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    train_df_encoded = pd.get_dummies(train_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "    # return: ì „ì²˜ë¦¬ëœ ë°ì´í„°, top/bottom ë²”ì£¼ ëª©ë¡ (í…ŒìŠ¤íŠ¸ì— í™œìš©)\n",
    "    return train_df_encoded, top_100_area_nm, bottom_100_area_nm, top_100_dist_nm, bottom_100_dist_nm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_df, top_areas, bottom_areas, top_dists, bottom_dists, reference_columns):\n",
    "    # ë™ì¼í•œ ì»¬ëŸ¼ ì œê±°\n",
    "    test_df = test_df.drop(['AREA_ID', 'DIST_CD'], axis=1)\n",
    "\n",
    "    # ë²”ì£¼ ì••ì¶•\n",
    "    test_df['AREA_NM'] = test_df['AREA_NM'].where(test_df['AREA_NM'].isin(top_areas) | test_df['AREA_NM'].isin(bottom_areas), 'Other')\n",
    "    test_df['DIST_NM'] = test_df['DIST_NM'].where(test_df['DIST_NM'].isin(top_dists) | test_df['DIST_NM'].isin(bottom_dists), 'Other')\n",
    "\n",
    "    # NaN ì²˜ë¦¬\n",
    "    num_cols = test_df.select_dtypes(include='number').columns\n",
    "    cat_cols = test_df.select_dtypes(include='object').columns\n",
    "\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].mean())\n",
    "    test_df[cat_cols] = test_df[cat_cols].fillna('Other')\n",
    "\n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    test_df_encoded = pd.get_dummies(test_df, columns=['AREA_NM', 'DIST_NM'], drop_first=True)\n",
    "\n",
    "    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì±„ìš°ê³  ìˆœì„œ ë§žì¶¤\n",
    "    missing_cols = set(reference_columns) - set(test_df_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        test_df_encoded[col] = 0\n",
    "\n",
    "    test_df_encoded = test_df_encoded[reference_columns]\n",
    "\n",
    "    return test_df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef51336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Repeat 1/42\n",
      " âž¤ Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 39.48096| val_rmse: 5.34528 |  0:00:03s\n",
      "epoch 1  | loss: 38.83648| val_rmse: 8.72288 |  0:00:07s\n",
      "epoch 2  | loss: 38.07483| val_rmse: 6.67571 |  0:00:11s\n",
      "epoch 3  | loss: 37.39881| val_rmse: 6.92304 |  0:00:15s\n",
      "epoch 4  | loss: 36.60314| val_rmse: 6.22639 |  0:00:19s\n",
      "epoch 5  | loss: 35.97552| val_rmse: 6.03657 |  0:00:23s\n",
      "epoch 6  | loss: 35.27765| val_rmse: 5.89713 |  0:00:27s\n",
      "epoch 7  | loss: 34.47963| val_rmse: 6.13468 |  0:00:31s\n",
      "epoch 8  | loss: 33.69906| val_rmse: 6.22045 |  0:00:36s\n",
      "epoch 9  | loss: 32.98782| val_rmse: 6.19184 |  0:00:40s\n",
      "epoch 10 | loss: 32.11109| val_rmse: 6.11273 |  0:00:43s\n",
      "epoch 11 | loss: 31.3971 | val_rmse: 6.06413 |  0:00:47s\n",
      "epoch 12 | loss: 30.56873| val_rmse: 6.03026 |  0:00:51s\n",
      "epoch 13 | loss: 29.69859| val_rmse: 5.95094 |  0:00:55s\n",
      "epoch 14 | loss: 28.72013| val_rmse: 5.8878  |  0:01:00s\n",
      "epoch 15 | loss: 27.91503| val_rmse: 5.78697 |  0:01:05s\n",
      "epoch 16 | loss: 26.88939| val_rmse: 5.77746 |  0:01:09s\n",
      "epoch 17 | loss: 25.85254| val_rmse: 5.649   |  0:01:13s\n",
      "epoch 18 | loss: 25.08571| val_rmse: 5.57322 |  0:01:18s\n",
      "epoch 19 | loss: 24.27272| val_rmse: 5.53329 |  0:01:23s\n",
      "epoch 20 | loss: 23.22005| val_rmse: 5.39113 |  0:01:27s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_rmse = 5.34528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39miloc[val_idx]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtabnet_params)\n\u001b[1;32m---> 63\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     64\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train, y_train\u001b[38;5;241m=\u001b[39my_train,\n\u001b[0;32m     65\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m     66\u001b[0m     eval_name\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     67\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     68\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     69\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     70\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, virtual_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     71\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     72\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     76\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, preds, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:278\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_importance:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# compute feature importance once the best model is defined\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_importances_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_feature_importances(X_train)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:759\u001b[0m, in \u001b[0;36mTabModel._compute_feature_importances\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_feature_importances\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute global feature importance.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     M_explain, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(X, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    760\u001b[0m     sum_explain \u001b[38;5;241m=\u001b[39m M_explain\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    761\u001b[0m     feature_importances_ \u001b[38;5;241m=\u001b[39m sum_explain \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sum_explain)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:353\u001b[0m, in \u001b[0;36mTabModel.explain\u001b[1;34m(self, X, normalize)\u001b[0m\n\u001b[0;32m    345\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    346\u001b[0m         PredictDataset(X),\n\u001b[0;32m    347\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    348\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m     )\n\u001b[0;32m    351\u001b[0m res_explain \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    354\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    356\u001b[0m     M_explain, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mforward_masks(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:283\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "np.random.seed(0)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv('TRAIN_DATA.csv', encoding='euc-kr')\n",
    "test_df = pd.read_csv('TEST_DATA.csv', encoding='euc-kr')\n",
    "\n",
    "# ì „ì²˜ë¦¬ (í•¨ìˆ˜ ì´ìš©)\n",
    "train_data_processed, top_areas, bottom_areas, top_dists, bottom_dists = preprocess_train_data(train_df)\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜\n",
    "train_data_processed['TOTAL_ELEC'] = np.log1p(train_data_processed['TOTAL_ELEC'])\n",
    "\n",
    "# ì˜ˆì¸¡ìš© X, y\n",
    "X = train_data_processed.drop(columns='TOTAL_ELEC')\n",
    "y = train_data_processed['TOTAL_ELEC']\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "test_data_processed = preprocess_test_data(test_df, top_areas, bottom_areas, top_dists, bottom_dists, X.columns)\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”©ì„ í–ˆê¸° ë•Œë¬¸ì— ë²”ì£¼í˜• ì¸ë±ìŠ¤ëŠ” ë¹„ì›Œë‘ \n",
    "cat_idxs = []\n",
    "cat_dims = []\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "tabnet_params = {\n",
    "    \"cat_idxs\": cat_idxs,\n",
    "    \"cat_dims\": cat_dims,\n",
    "    \"cat_emb_dim\": 1,\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": dict(lr=1e-4),\n",
    "    \"scheduler_params\": {\"step_size\": 50, \"gamma\": 0.9},\n",
    "    \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n",
    "    \"mask_type\": \"entmax\",\n",
    "    \"gamma\": 1.3,\n",
    "    #\"loss_fn\": \"logcosh\"\n",
    "}\n",
    "\n",
    "# ë°˜ë³µ ë° K-Fold ì„¤ì •\n",
    "num_repeats = 42\n",
    "num_folds = 5\n",
    "rmse_scores = []\n",
    "y_pred_final = []  # ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for repeat in range(num_repeats):\n",
    "    print(f\"\\n Repeat {repeat + 1}/{num_repeats}\")\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=repeat)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\" âž¤ Fold {fold + 1}/{num_folds}\")\n",
    "        X_train, X_val = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "        y_train, y_val = y.iloc[train_idx].values.reshape(-1, 1), y.iloc[val_idx].values.reshape(-1, 1)\n",
    "\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_name=['val'],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=100,\n",
    "            patience=20,\n",
    "            batch_size=256, virtual_batch_size=64,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val).flatten()\n",
    "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "# ì „ì²´ ë°˜ë³µì´ ëë‚œ í›„, y_pred_finalì— ëª¨ë“  ì˜ˆì¸¡ê°’ì´ ì €ìž¥ë¨\n",
    "y_pred_final = np.concatenate(y_pred_final, axis=0)  # ëª¨ë“  foldì™€ repeatì—ì„œ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì„ ê²°í•©\n",
    "\n",
    "print(f\"\\n í‰ê·  RMSE (Across {num_repeats} repeats x {num_folds} folds): {np.mean(rmse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab9eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í‰ê·  RMSE (Across 42 repeats x 5 folds): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\sally\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nâœ… í‰ê·  RMSE (Across {num_repeats} repeats x {num_folds} folds): {np.mean(rmse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf7ccf",
   "metadata": {},
   "source": [
    "#### âœ… ìƒê¶Œë³„ ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„ (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2b8ec92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ê°’ ê³„ì‚°\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_pred_final \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ y_pred_finalì— ì €ìž¥\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°\u001b[39;00m\n\u001b[0;32m      8\u001b[0m error_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:310\u001b[0m, in \u001b[0;36mTabModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    304\u001b[0m         PredictDataset(X),\n\u001b[0;32m    305\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    306\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    309\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    311\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    312\u001b[0m     output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(data)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\utils.py:78\u001b[0m, in \u001b[0;36mPredictDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[index]\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\sally\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "y_pred_final = model.predict(X).flatten()  # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ y_pred_finalì— ì €ìž¥\n",
    "\n",
    "# ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°\n",
    "error_df = train_df.copy()\n",
    "error_df['predicted'] = y_pred_final\n",
    "error_df['abs_error'] = np.abs(error_df['TOTAL_ELEC'] - error_df['predicted'])\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ìƒê¶Œ í‘œì‹œ (ì˜¤ì°¨ ê¸°ì¤€)\n",
    "top_errors = error_df.sort_values('abs_error', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_errors, x='AREA_NM', y='abs_error', palette='Reds_r')\n",
    "plt.title(\"ìƒê¶Œë³„ ì˜ˆì¸¡ ì˜¤ì°¨ ìƒìœ„ 10\")\n",
    "plt.xlabel(\"ìƒê¶Œ ëª…\")\n",
    "plt.ylabel(\"ì ˆëŒ€ ì˜¤ì°¨ (abs_error)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571824b",
   "metadata": {},
   "source": [
    "#### âœ… ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ ìž¬í•™ìŠµ\n",
    "- ì „ì²´ X, yë¡œ ìµœì¢… TabNet ëª¨ë¸ ìž¬í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04206c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ\n",
    "final_model = TabNetRegressor(**tabnet_params)\n",
    "final_model.fit(\n",
    "    X_train=X.values, y_train=y.values,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "y_pred = final_model.predict(test_data_processed.values).flatten()\n",
    "\n",
    "# ê²°ê³¼ DataFrame ìƒì„±\n",
    "submission = pd.DataFrame({'y_pred': y_pred})\n",
    "\n",
    "# ì €ìž¥\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ 'submission.csv' ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
